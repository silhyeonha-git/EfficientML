2.1 트랜스포머 아키텍처란
- 트랜스포머는 학습 속도가 빨라짐. 추론 속도는?
- RNN에서 토큰의 정보가 희석되는 이유 + 층을 깊이 쌓으면 gradient 소실/증폭 발생 정확한 이유
2.2 텍스트를 임베딩으로 변환하기
- 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정하는 서브워드 토큰화는 학습 dataset을 기준으로 단위가 결정되는건가?
- 임베딩, 인코딩 용어 익혀두기
- nn.Embedding, nn.Linear차이?
- "임베딩 층이 학습데이터로 훈련되어서 단어의 의미를 담는다"는 지점에서 딥러닝이 기존 머신러닝과 차별화되는데, 
    딥러닝에서는 모델이 특정 작업을 잘 수행하도록 학습하는 과정에서 데이터의 의미를 잘 담은 임베딩을 만드는 방법도 함께 학습한다?
    이게 머신러닝이랑 딥러닝 차이점??
- 토큰 임베딩에 위치 인코딩값을 단순 덧셈해도 되는 이유? 더해버리는데 어떻게 위치를 인식할 수 있는거지?
2.3 어텐션 이해하기
- 임베딩을 직접 활용해 관련도를 계산하는 방식은 2가지 문제 발생함. 
    같은 단어는 크게 계산되면서 주변 맥락 반영 못하는 경우 발생.
    문법으로 토큰이 이어지는 경우 관련성 반영 어려움.(ex. 나는, 최근)
- value는 왜 w가 필요하지? 실제 input_embedding으로 하면 안되나?
- 쿼리, 키, 벨류가 유사한값을 의미하는 것으로 생각되는데, 각각 w가 별도로 있는 이유?
- Q@K 이후, sqrt(embedding_dim)으로 나누는 이유? 분산이 커지는 것을 방지하기 위해?? 하필 왜저값?
- 